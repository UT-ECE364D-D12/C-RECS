{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tdutton/mambaforge/envs/cers/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_name: str = \"meta-llama/Meta-Llama-3-8B-Instruct\",) -> tuple[AutoModelForCausalLM, AutoTokenizer]:\n",
    "    config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=False,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "    )\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, low_cpu_mem_usage=True, quantization_config=config)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:06<00:00,  2.03s/it]\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "model, tokenizer = load_model(\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
    "# model, tokenizer = load_model(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "# model, tokenizer = load_model(\"google/gemma-7b-it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'd like to watch a romantic, melodramatic movie.\n"
     ]
    }
   ],
   "source": [
    "def generate_prompt(movie: str) -> str:\n",
    "    return f\"\"\"You are interacting with a movie recommendation system. Your goal is to make a short request that will help the system to suggest the movie \"{movie}\" without mentioning its title, characters, or ANY plot elements. The response should instead use GENERAL characteristics like the genre, tone, and themes of the movie. Your request should be extremely short, sound conversational, not be too enthusiastic, and use informal wording. As an example, for the movie \"La La Land\" you should give a request like \"I want to watch a rom com.\" Reply ONLY with the human-like request for a movie. DO NOT include any other text.\n",
    "    \"\"\"\n",
    "    \n",
    "# SPLIT_STR = \"\\nmodel\\n\" # Gemma\n",
    "SPLIT_STR = \"[/INST] \" # Minstral & Llama-2\n",
    "\n",
    "# Form prompt\n",
    "chat = [{\"role\": \"user\", \"content\": generate_prompt(\"The Notebook\")}]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "# Tokenize\n",
    "input_tokens = tokenizer.encode(prompt, add_special_tokens=False, return_tensors=\"pt\")\n",
    "\n",
    "# Generate\n",
    "output_tokens = model.generate(input_tokens.to(model.device), max_new_tokens=150, do_sample=True, pad_token_id=tokenizer.eos_token_id)[0]\n",
    "\n",
    "# Decode\n",
    "response = tokenizer.decode(output_tokens, skip_special_tokens=True).split(SPLIT_STR)[-1]\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    1,   733, 16289, 28793,   995,   460,   264,  1338, 14113,   288,\n",
       "           395,   264,  5994, 26077,  1587, 28723,  3604,  5541,   349,   298,\n",
       "          1038,   264,  2485,  2159,   369,   622,  1316,   272,  1587,   298,\n",
       "          3397,   272,  5994,   345,  1981,  1067,   811,   978,  3690,   366,\n",
       "           768, 28739,  1671,  4389,   288,   871,  3941, 28725,  6128, 28725,\n",
       "           442,  5788,  9242,  5176, 28723,   415,  2899,  1023,  3519,   938,\n",
       "         25778,   725,  1086, 15559,   737,   272, 15926, 28725, 10294, 28725,\n",
       "           304, 18978,   302,   272,  5994, 28723,  3604,  2159,  1023,   347,\n",
       "          3078,   864, 28725,  2622,  5315,  1249, 28725,   304,   459,   347,\n",
       "          1368, 18185,  3953, 28723,  1263,  2757, 28725,   272,  7918,  5994,\n",
       "           345, 28743,  7853,   662,  8064,  7481, 28739,  1023,  2111,   264,\n",
       "          2159,   737,   345, 28737,   947,   298,  3054,   264,  5423,   432,\n",
       "           611,  3357,   346,  9688,  9880,   395,   272,  2930, 28733,  4091,\n",
       "          2159,   354,   264,  5994, 28723,  9317,  5457,  3024,   707,   799,\n",
       "          2245, 28723,    13,   260,   733, 28748, 16289, 28793]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tokens"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
