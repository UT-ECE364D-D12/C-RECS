{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tdutton/mambaforge/envs/cers/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_name: str = \"meta-llama/Meta-Llama-3-8B-Instruct\",) -> tuple[AutoModelForCausalLM, AutoTokenizer]:\n",
    "    config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=False,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "    )\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, low_cpu_mem_usage=True, quantization_config=config)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [01:08<00:00, 22.88s/it]\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "model, tokenizer = load_model(\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
    "# model, tokenizer = load_model(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "# model, tokenizer = load_model(\"google/gemma-7b-it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'd appreciate a thought-provoking drama with themes of hope, resilience, and redemption. Any recommendations?\n"
     ]
    }
   ],
   "source": [
    "def generate_prompt(movie: str) -> str:\n",
    "    return f\"\"\"You are a person interacting with a movie recommendation system. Your goal is to make a short request that will help the system to suggest the movie \"{movie}\" without mentioning its title, characters, or ANY plot elements. The response should instead use GENERAL characteristics like the genre, tone, and themes of the movie. Your request should be concise, sound conversational, and not be too enthusiastic. For example, the hidden movie \"Crazy Stupid Love\" should give a request like \"I'm looking for a silly romantic comedy with a happy ending. Any suggestions?\" Reply ONLY with the human-like request for a movie. DO NOT include any other text.\n",
    "    \"\"\"\n",
    "    \n",
    "# SPLIT_STR = \"\\nmodel\\n\" # Gemma\n",
    "SPLIT_STR = \"[/INST] \" # Minstral & Llama-2\n",
    "\n",
    "# Form prompt\n",
    "chat = [\n",
    "    { \"role\": \"user\", \"content\": generate_prompt(\"Shawshank Redemption\") },\n",
    "]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "# Tokenize\n",
    "input_tokens = tokenizer.encode(prompt, add_special_tokens=False, return_tensors=\"pt\")\n",
    "\n",
    "# Generate\n",
    "output_tokens = model.generate(input_tokens.to(model.device), max_new_tokens=150, do_sample=True, pad_token_id=tokenizer.eos_token_id)[0]\n",
    "\n",
    "# Decode\n",
    "response = tokenizer.decode(output_tokens, skip_special_tokens=True).split(SPLIT_STR)[-1]\n",
    "\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
