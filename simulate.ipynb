{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference Notebook for Prompt Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tdutton/mambaforge/envs/cers/lib/python3.10/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.7.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "/home/tdutton/mambaforge/envs/cers/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BertModel, BertTokenizer, BitsAndBytesConfig, MistralForCausalLM\n",
    "import logging\n",
    "from typing import List, Callable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_language_model(model_name: str = \"google/gemma-7b-it\") -> tuple[AutoModelForCausalLM, AutoTokenizer]:\n",
    "    config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=False,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "    )\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name, \n",
    "        low_cpu_mem_usage=True, \n",
    "        quantization_config=config, \n",
    "        attn_implementation=\"flash_attention_2\"\n",
    "    )\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token if tokenizer.pad_token is None else tokenizer.pad_token\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "def build_encoder():\n",
    "    loggers = [logging.getLogger(name) for name in logging.root.manager.loggerDict]\n",
    "    for logger in loggers:\n",
    "        if \"transformers\" in logger.name.lower():\n",
    "            logger.setLevel(logging.ERROR)\n",
    "            \n",
    "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    model = BertModel.from_pretrained(\"bert-base-uncased\").cuda()\n",
    "\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_generators = [\n",
    "    lambda movie: f\"\"\"You are interacting with a movie recommendation system. Your goal is to make a short request that will help the system to suggest the movie \"{movie}\" without mentioning its title, characters, or ANY plot elements. The response should instead use GENERAL characteristics like the genre, tone, and themes of the movie. Your request should be concise, sound conversational, and not be too enthusiastic. As an example, for the movie \"Crazy Stupid Love\" you should give a request like \"I'm looking for a silly romantic comedy with a happy ending. Any suggestions?\" Reply ONLY with the human-like request for a movie. DO NOT include any other text.\"\"\",\n",
    "    lambda movie: f\"\"\"You are interacting with a movie recommendation system. Your goal is to make a short request that will help the system to suggest the movie \"{movie}\" without mentioning its title, characters, or ANY plot elements. The response should instead use GENERAL characteristics like the genre, tone, and themes of the movie. Your request should be extremely short, sound conversational, not be too enthusiastic, and use informal wording. As an example, for the movie \"La La Land\" you should give a request like \"I want to watch a rom com.\" Reply ONLY with the human-like request for a movie. DO NOT include any other text.\"\"\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimulatorDataset(Dataset):\n",
    "    def __init__(self, movies: pd.DataFrame, tokenizer: AutoTokenizer, prompt_generators: List[Callable]) -> None:\n",
    "        self.movies = movies\n",
    "        self.tokenizer = tokenizer\n",
    "        self.prompt_generators = prompt_generators\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.movies) * len(self.prompt_generators)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        prompt_idx, movie_idx = divmod(idx, len(self.movies))\n",
    "        movie_id, movie_title = self.movies.iloc[movie_idx][[\"movie_id\", \"movie_title\"]]\n",
    "\n",
    "        # Generate the prompt for the movie\n",
    "        prompt = self.prompt_generators[prompt_idx](movie_title)\n",
    "\n",
    "        # Form prompt\n",
    "        chat = [{\"role\": \"user\", \"content\": prompt}]\n",
    "\n",
    "        # Apply the chat template\n",
    "        prompt = self.tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "        return movie_id, movie_title, prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLIT_STR = \"[/INST] \" # Minstral & Llama-2\n",
    "\n",
    "def simulate(\n",
    "    language_model: MistralForCausalLM,\n",
    "    language_tokenizer: AutoTokenizer,\n",
    "    encoder_model: BertModel,\n",
    "    encoder_tokenizer: BertTokenizer,\n",
    "    dataloader: DataLoader,\n",
    "    max_length: int = 2048,\n",
    ") -> pd.DataFrame:\n",
    "    data = pd.DataFrame(columns=[\"movie_id\", \"movie_title\", \"request\", \"encoded_request\"])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for movie_ids, movie_titles, prompts in tqdm(dataloader, desc=\"Simulating\", unit=\"batch\"):\n",
    "            # Tokenize (llm)\n",
    "            input_tokens = language_tokenizer(prompts, add_special_tokens=False, padding=True, return_tensors=\"pt\").to(language_model.device)\n",
    "\n",
    "            # Generate request\n",
    "            request_tokens = language_model.generate(**input_tokens, max_new_tokens=max_length, do_sample=True, pad_token_id=language_tokenizer.eos_token_id)\n",
    "\n",
    "            # Decode request\n",
    "            batch_requests = [language_tokenizer.decode(output, skip_special_tokens=True).split(SPLIT_STR)[-1] for output in request_tokens]\n",
    "\n",
    "            # Tokenize (bert)\n",
    "            encoder_tokens = encoder_tokenizer(batch_requests, padding=True, return_tensors=\"pt\").to(language_model.device)\n",
    "\n",
    "            # Encode request, grab the CLS token\n",
    "            batch_encoded_requests = encoder_model(**encoder_tokens)\n",
    "\n",
    "            encoded_requests = [hidden_state[0].cpu().numpy() for hidden_state in batch_encoded_requests.last_hidden_state]\n",
    "\n",
    "            batch_requests = pd.DataFrame({\n",
    "                \"movie_id\": movie_ids,\n",
    "                \"movie_title\": movie_titles,\n",
    "                \"request\": batch_requests,\n",
    "                \"encoded_request\": encoded_requests\n",
    "            })\n",
    "\n",
    "            data = pd.concat([data, batch_requests], ignore_index=True)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model and tokenizer\n",
    "language_model, language_tokenizer = build_language_model(\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
    "\n",
    "encoder_model, encoder_tokenizer = build_encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the set Vof movies\n",
    "movies = pd.read_csv(\"data/ml-100k/u.item\", sep=\"|\", encoding=\"latin-1\", header=None, names=[\"movie_id\", \"movie_title\", \"release_date\", \"url\", \"unknown\", \"Action\", \"Adventure\", \"Animation\", \"Children's\", \"Comedy\", \"Crime\", \"Documentary\", \"Drama\", \"Fantasy\", \"Film-Noir\", \"Horror\", \"Musical\", \"Mystery\", \"Romance\", \"Sci-Fi\", \"Thriller\", \"War\", \"Western\"])\n",
    "\n",
    "movies = movies[[\"movie_id\", \"movie_title\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataset\n",
    "dataset = SimulatorDataset(movies, language_tokenizer, prompt_generators)\n",
    "\n",
    "# Create the dataloader\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate the responses\n",
    "data = simulate(language_model, language_tokenizer, encoder_model, encoder_tokenizer, dataloader)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the new dataframe\n",
    "data.to_hdf(\"data/requests.h5\", key=\"df\", mode=\"w\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
