{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference Notebook for Prompt Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tdutton/mambaforge/envs/cers/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BertModel, BertTokenizer, BitsAndBytesConfig, MistralForCausalLM\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_language_model(model_name: str = \"google/gemma-7b-it\") -> tuple[AutoModelForCausalLM, AutoTokenizer]:\n",
    "    config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=False,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "    )\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name, \n",
    "        low_cpu_mem_usage=True, \n",
    "        quantization_config=config, \n",
    "        attn_implementation=\"flash_attention_2\"\n",
    "    )\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token if tokenizer.pad_token is None else tokenizer.pad_token\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "def load_encoder():\n",
    "    loggers = [logging.getLogger(name) for name in logging.root.manager.loggerDict]\n",
    "    for logger in loggers:\n",
    "        if \"transformers\" in logger.name.lower():\n",
    "            logger.setLevel(logging.ERROR)\n",
    "            \n",
    "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    model = BertModel.from_pretrained(\"bert-base-uncased\").cuda()\n",
    "\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(movie: str) -> str:\n",
    "    return f\"\"\"You are a person interacting with a movie recommendation system. Your goal is to make a short request that will help the system to suggest the movie \"{movie}\" without mentioning its title, characters, or ANY plot elements. The response should instead use GENERAL characteristics like the genre, tone, and themes of the movie. Your request should be concise, sound conversational, and not be too enthusiastic. For example, the hidden movie \"Crazy Stupid Love\" should give a request like \"I'm looking for a silly romantic comedy with a happy ending. Any suggestions?\" Reply ONLY with the human-like request for a movie. DO NOT include any other text.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimulatorDataset(Dataset):\n",
    "    def __init__(self, movies: pd.DataFrame, tokenizer: AutoTokenizer) -> None:\n",
    "        self.movies = movies\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.movies)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Generate the prompt for the movie\n",
    "        prompt = generate_prompt(self.movies.iloc[idx][\"movie_title\"])\n",
    "\n",
    "        # Form prompt\n",
    "        chat = [{\"role\": \"user\", \"content\": prompt}]\n",
    "\n",
    "        # Apply the chat template\n",
    "        prompt = self.tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "        return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLIT_STR = \"[/INST] \" # Minstral & Llama-2\n",
    "\n",
    "def simulate(\n",
    "    language_model: MistralForCausalLM,\n",
    "    language_tokenizer: AutoTokenizer,\n",
    "    encoder_model: BertModel,\n",
    "    encoder_tokenizer: BertTokenizer,\n",
    "    dataloader: DataLoader,\n",
    "    max_length: int = 2048,\n",
    ") -> tuple[list, list]:\n",
    "    requests = []\n",
    "    encoded_requests = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Simulating\", unit=\"batch\"):\n",
    "            # Tokenize (llm)\n",
    "            input_tokens = language_tokenizer(batch, add_special_tokens=False, padding=True, return_tensors=\"pt\").to(language_model.device)\n",
    "\n",
    "            # Generate request\n",
    "            request_tokens = language_model.generate(**input_tokens, max_new_tokens=max_length, do_sample=True, pad_token_id=language_tokenizer.eos_token_id)\n",
    "\n",
    "            # Decode request\n",
    "            batch_requests = [language_tokenizer.decode(output, skip_special_tokens=True).split(SPLIT_STR)[-1] for output in request_tokens]\n",
    "            requests.extend(batch_requests)\n",
    "\n",
    "            # Tokenize (bert)\n",
    "            encoder_tokens = encoder_tokenizer(batch_requests, padding=True, return_tensors=\"pt\").to(language_model.device)\n",
    "\n",
    "            batch_encoded_requests = encoder_model(**encoder_tokens)\n",
    "\n",
    "            # Encode request, grab the CLS token\n",
    "            encoded_requests.extend([hidden_state[0].cpu().numpy() for hidden_state in batch_encoded_requests.last_hidden_state])\n",
    "\n",
    "    return requests, encoded_requests "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards: 100%|██████████| 3/3 [00:00<00:00, 10.67it/s]\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:09<00:00,  3.05s/it]\n",
      "/home/tdutton/mambaforge/envs/cers/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load the model and tokenizer\n",
    "language_model, language_tokenizer = load_language_model(\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
    "\n",
    "encoder_model, encoder_tokenizer = load_encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the set Vof movies\n",
    "movies = pd.read_csv(\"data/ml-100k/u.item\", sep=\"|\", encoding=\"latin-1\", header=None, names=[\"movie_id\", \"movie_title\", \"release_date\", \"url\", \"unknown\", \"Action\", \"Adventure\", \"Animation\", \"Children's\", \"Comedy\", \"Crime\", \"Documentary\", \"Drama\", \"Fantasy\", \"Film-Noir\", \"Horror\", \"Musical\", \"Mystery\", \"Romance\", \"Sci-Fi\", \"Thriller\", \"War\", \"Western\"])\n",
    "\n",
    "movies = movies[[\"movie_id\", \"movie_title\"]]\n",
    "\n",
    "movies = movies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataset\n",
    "dataset = SimulatorDataset(movies, language_tokenizer)\n",
    "\n",
    "# Create the dataloader\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Simulating: 100%|██████████| 5/5 [00:08<00:00,  1.62s/batch]\n"
     ]
    }
   ],
   "source": [
    "# Simulate the responses\n",
    "requests, encoded_requests = simulate(language_model, language_tokenizer, encoder_model, encoder_tokenizer, dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie_id</th>\n",
       "      <th>movie_title</th>\n",
       "      <th>request</th>\n",
       "      <th>encoded_request</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>I'd appreciate a family-friendly animated movi...</td>\n",
       "      <td>[-0.08137988, -0.33736423, -0.15107499, -0.215...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>GoldenEye (1995)</td>\n",
       "      <td>I'm looking for a thrilling espionage film wit...</td>\n",
       "      <td>[-0.06589088, -0.39797422, -0.258162, -0.51747...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Four Rooms (1995)</td>\n",
       "      <td>I'd appreciate a recommendation for an antholo...</td>\n",
       "      <td>[-0.054674856, 0.018768324, -0.26771954, -0.12...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Get Shorty (1995)</td>\n",
       "      <td>I'd appreciate a crime comedy with a clever sc...</td>\n",
       "      <td>[0.004236746, -0.27087268, -0.3016517, -0.3320...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Copycat (1995)</td>\n",
       "      <td>I'm in the mood for a thought-provoking psycho...</td>\n",
       "      <td>[-0.10669358, -0.3234886, -0.0020956919, -0.32...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   movie_id        movie_title  \\\n",
       "0         1   Toy Story (1995)   \n",
       "1         2   GoldenEye (1995)   \n",
       "2         3  Four Rooms (1995)   \n",
       "3         4  Get Shorty (1995)   \n",
       "4         5     Copycat (1995)   \n",
       "\n",
       "                                             request  \\\n",
       "0  I'd appreciate a family-friendly animated movi...   \n",
       "1  I'm looking for a thrilling espionage film wit...   \n",
       "2  I'd appreciate a recommendation for an antholo...   \n",
       "3  I'd appreciate a crime comedy with a clever sc...   \n",
       "4  I'm in the mood for a thought-provoking psycho...   \n",
       "\n",
       "                                     encoded_request  \n",
       "0  [-0.08137988, -0.33736423, -0.15107499, -0.215...  \n",
       "1  [-0.06589088, -0.39797422, -0.258162, -0.51747...  \n",
       "2  [-0.054674856, 0.018768324, -0.26771954, -0.12...  \n",
       "3  [0.004236746, -0.27087268, -0.3016517, -0.3320...  \n",
       "4  [-0.10669358, -0.3234886, -0.0020956919, -0.32...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add the responses to the dataframe\n",
    "movies[\"request\"] = requests\n",
    "movies[\"encoded_request\"] = encoded_requests\n",
    "\n",
    "movies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_11325/1699144900.py:2: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block1_values] [items->Index(['movie_title', 'request', 'encoded_request'], dtype='object')]\n",
      "\n",
      "  movies.to_hdf(\"data/requests.h5\", key=\"df\", mode=\"w\", index=False)\n"
     ]
    }
   ],
   "source": [
    "# Save the new dataframe\n",
    "movies.to_hdf(\"data/requests.h5\", key=\"df\", mode=\"w\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
